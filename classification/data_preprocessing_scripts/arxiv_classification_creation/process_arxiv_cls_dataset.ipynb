{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lamdo/miniconda3/envs/venv310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json, re\n",
    "from datasets import load_dataset, load_from_disk, Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_arxiv_id_with_regex(arxiv_id):\n",
    "    pattern = r\"^arXiv:(\\d+\\.\\d+)(?:v\\d+)?$\"\n",
    "    match = re.match(pattern, arxiv_id)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return arxiv_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/scratch/lamdo/arxiv_dataset/arxivid2metadata.json\") as f:\n",
    "    arxivid2metadata = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ds = load_from_disk(\"/scratch/lamdo/arxiv_classification/arxiv_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 8, 'arxiv_id': 'arXiv:1611.03253v1'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = {}\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    original_split_arxiv_ids = ds[split][\"arxiv_id\"]\n",
    "    labels = ds[split][\"label\"]\n",
    "\n",
    "    split_arxiv_ids = [process_arxiv_id_with_regex(arxiv_id) for arxiv_id in original_split_arxiv_ids]\n",
    "    metadata = [arxivid2metadata.get(arxiv_id) for arxiv_id in split_arxiv_ids]\n",
    "\n",
    "    out = []\n",
    "    for i in range(len(split_arxiv_ids)):\n",
    "        if not metadata[i]:\n",
    "            continue\n",
    "        label = labels[i]\n",
    "        arxiv_id = original_split_arxiv_ids[i]\n",
    "        title = re.sub('\\s+', ' ', metadata[i][\"title\"].replace(\"\\n\", \" \"))\n",
    "        abstract = re.sub('\\s+', ' ', metadata[i][\"abstract\"].replace(\"\\n\", \" \"))\n",
    "        out.append({\"label\": label, \"arxiv_id\": arxiv_id, \"title\": title, \"abstract\": abstract})\n",
    "    processed_dataset[split] = out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = DatasetDict({\n",
    "    split: Dataset.from_list([{\"paper_id\": line[\"arxiv_id\"], \"label\": line[\"label\"]} for line in processed_dataset[split]]) for split in [\"train\", \"test\"]\n",
    "})\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"evaluation\": Dataset.from_dict(\n",
    "        {\"doc_id\": [line[\"arxiv_id\"] for line in processed_dataset[\"train\"] + processed_dataset[\"test\"]], \n",
    "         \"title\": [line[\"title\"] for line in processed_dataset[\"train\"] + processed_dataset[\"test\"]],\n",
    "         \"abstract\": [line[\"abstract\"] for line in processed_dataset[\"train\"] + processed_dataset[\"test\"]],\n",
    "         \"label\": [line[\"label\"] for line in processed_dataset[\"train\"] + processed_dataset[\"test\"]]}\n",
    "         ),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    evaluation: Dataset({\n",
       "        features: ['doc_id', 'title', 'abstract', 'label'],\n",
       "        num_rows: 27395\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 25200/25200 [00:00<00:00, 1304846.25 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 2195/2195 [00:00<00:00, 387870.63 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_test.save_to_disk(\"/scratch/lamdo/arxiv_classification/arxiv_data_t+a_test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 27395/27395 [00:00<00:00, 648092.76 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset.save_to_disk(\"/scratch/lamdo/arxiv_classification/arxiv_data_t+a/\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
