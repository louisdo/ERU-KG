# Documentation for model evaluation

## Keyphrase generation evaluation

To run evaluation for ERU-KG, please execute the following script for generating keyphrases

```bash
device_to_use=0
datasets=(
    "semeval" 
    "inspec" 
    "nus" 
    "krapivin" 
    "kp20k" 
)
result_folder="/YOUR/RESULT/FOLDER" # generated keyphrases will be stored here

models_types=(
    eru-kg-base
)
for dataset in "${datasets[@]}"; do
    for model_type in "${models_types[@]}"; do
        echo "Config: $dataset - $model_type - $top_k"

        CUDA_VISIBLE_DEVICES=$device_to_use DATASET_TO_USE=$dataset RESULTS_FOLDER=$result_folder MODEL_TO_USE=$model_type python run_keyphrase_prediction_batch.py
    done
done
```


After that, we need to run the evaluation script like so

```bash
models_to_include=(
    eru-kg-base
)

join_by() {
  local separator="$1"
  shift
  local first="$1"
  shift
  printf "%s" "$first" "${@/#/$separator}"
}

models_to_include_joined=$(join_by , "${models_to_include[@]}")

echo "$models_to_include_joined"

RESULTS_FOLDER="/YOUR/RESULT/FOLDER" \
DATASETS_TO_INCLUDE="semeval,inspec,nus,krapivin,kp20k" MODELS_TO_INCLUDE=$models_to_include_joined python view_kg_experiment_results.py
```

After the program terminates, the results should be in `view.csv`


## Text retrieval

In this paper, we test whether the keyphrases generated by ERU-KG is useful for retrieval tasks when employed as query and/or document expansion.

To conduct this experiment, we need to perform 3 steps: 1) Generate keyphrases for documents and/or queries; 2) Do indexing; 3) Run evaluation

### Generate keyphrases for documents and/or queries
Execute the following command

```bash
datasets=(
    "scifact" "scifact_queries"
    "scidocs" "scidocs_queries"
    "trec_covid" "trec_covid_queries"
    "nfcorpus" "nfcorpus_queries"
    "doris_mae" "doris_mae_queries"
    "acm_cr" "acm_cr_queries")
result_folder=/YOUR/RESULT/FOLDER # generated keyphrases will be stored here

# eru-kg (final ver) (neighborsize varying)
models_types=(
    eru-kg-base
)

for dataset in "${datasets[@]}"; do
    for model_type in "${models_types[@]}"; do
        echo "Config: $dataset - $model_type - $top_k"

        CUDA_VISIBLE_DEVICES=1 DATASET_TO_USE=$dataset RESULTS_FOLDER=$result_folder MODEL_TO_USE=$model_type python run_keyphrase_prediction_batch.py
    done
done
```

### Do indexing
First we need to change directory


```bash
cd text_retrieval/src/build_index
```


Then, run the following command for indexing
```bash
datasets=(
    scifact
    scidocs
    trec_covid
    nfcorpus
    doris_mae
    acm_cr
)

for dataset in "${datasets[@]}"; do
    # the first command is for creating index WITHOUT document expansion (this is necessary for evaluating the setting where only query expansion is employed)
    OUTPUT_FOLDER="/YOUR/EXPERIMENTS/FOLDER/${dataset}_collections/pyserini_formatted_collection" \
    INDEX_FOLDER="/YOUR/EXPERIMENTS/FOLDER/index/${dataset}" \
    python ${dataset}.py


    # the second command is for creating index WITH document expansion
    OUTPUT_FOLDER="/YOUR/EXPERIMENTS/FOLDER/${dataset}_collections/pyserini_formatted_collection_eru-kg-base" \
    KEYWORD_FOR_DOCUMENT_EXPANSION="/YOUR/RESULT/FOLDER/${dataset}--eru-kg-base.json" \
    INDEX_FOLDER="/YOUR/EXPERIMENTS/FOLDER/index/${dataset}_keyphrase_expansion_eru-kg-base" \
    EXPANSION_ONLY_PRESENT_KEYPHRASES=0 \
    NUMBER_OF_KEYWORDS_EACH_TYPE=10 \
    python ${dataset}.py

done
```

### Run evaluation

First we need to change directory


```bash
cd text_retrieval/src/
```

In our experiments, we evaluate three settings *Query* (do query expansion only), *Doc* (do document expansion only) and *Both* (apply both query and document expansion). Run the following command

```bash
datasets=(
    scidocs
    scifact
    trec_covid
    nfcorpus
    doris_mae
    acm_cr
)

for dataset in "${datasets[@]}"; do
    # Evaluation of *Query* setting
    INDEX_PATH="/YOUR/EXPERIMENTS/FOLDER/index/${dataset}" \
    EXPERIMENT_NAME="${dataset}_keyphrase_expansion_eru-kg-base [query expansion]" \
    GROUNDTRUTH_DATA_PATH="" DATASET_NAME="${dataset}" \
    QUERY_EXPANSION_PATH="/YOUR/RESULT/FOLDER/${dataset}_queries--eru-kg-base.json" \
    python bm25_search_v3.py

    # Evaluation of *Doc* setting
    INDEX_PATH="/YOUR/EXPERIMENTS/FOLDER/index/${dataset}_keyphrase_expansion_eru-kg-base" \
    EXPERIMENT_NAME="${dataset}_keyphrase_expansion_eru-kg-base [doc expansion]" \
    GROUNDTRUTH_DATA_PATH="" DATASET_NAME="${dataset}" \
    python bm25_search_v3.py

    # Evaluation of *Both* setting
    INDEX_PATH="/YOUR/EXPERIMENTS/FOLDER/index/${dataset}_keyphrase_expansion_eru-kg-base" \
    EXPERIMENT_NAME="${dataset}_keyphrase_expansion_eru-kg-base [query + doc expansion]" \
    GROUNDTRUTH_DATA_PATH="" DATASET_NAME="${dataset}" \
    QUERY_EXPANSION_PATH="/YOUR/RESULT/FOLDER/${dataset}_queries--eru-kg-base.json" \
    python bm25_search_v3.py
done
```

After the programs terminate the results should be in `bm25_eval_results.txt`