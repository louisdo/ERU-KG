{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lamdo/miniconda3/envs/venv310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: google/bert_uncased_L-6_H-512_A-8\n",
      "Size: 133.78 MB\n",
      "Number of parameters: 35,068,416\n",
      "\n",
      "Model Configuration:\n",
      "Hidden Size: 512\n",
      "Number of Hidden Layers: 6\n",
      "Number of Attention Heads: 8\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertConfig\n",
    "import torch\n",
    "\n",
    "def get_model_size(model):\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "    \n",
    "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "    return size_all_mb\n",
    "\n",
    "# Load the model\n",
    "model_name = \"google/bert_uncased_L-6_H-512_A-8\"\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# Get the model size\n",
    "model_size_mb = get_model_size(model)\n",
    "\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Size: {model_size_mb:.2f} MB\")\n",
    "\n",
    "# Get the number of parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters: {num_params:,}\")\n",
    "\n",
    "# Print model configuration\n",
    "config = model.config\n",
    "print(\"\\nModel Configuration:\")\n",
    "print(f\"Hidden Size: {config.hidden_size}\")\n",
    "print(f\"Number of Hidden Layers: {config.num_hidden_layers}\")\n",
    "print(f\"Number of Attention Heads: {config.num_attention_heads}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: distilbert-base-uncased\n",
      "Size: 253.16 MB\n",
      "Number of parameters: 66,362,880\n",
      "\n",
      "Model Configuration:\n",
      "Hidden Size: 768\n",
      "Number of Hidden Layers: 6\n",
      "Number of Attention Heads: 12\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertModel, DistilBertConfig\n",
    "import torch\n",
    "\n",
    "def get_model_size(model):\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "    \n",
    "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "    return size_all_mb\n",
    "\n",
    "# Load the model\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model = DistilBertModel.from_pretrained(model_name)\n",
    "\n",
    "# Get the model size\n",
    "model_size_mb = get_model_size(model)\n",
    "\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Size: {model_size_mb:.2f} MB\")\n",
    "\n",
    "# Get the number of parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters: {num_params:,}\")\n",
    "\n",
    "# Print model configuration\n",
    "config = model.config\n",
    "print(\"\\nModel Configuration:\")\n",
    "print(f\"Hidden Size: {config.hidden_size}\")\n",
    "print(f\"Number of Hidden Layers: {config.num_hidden_layers}\")\n",
    "print(f\"Number of Attention Heads: {config.num_attention_heads}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1351679/1741788588.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  CKPT = torch.load(CKPT_PATH, map_location = torch.device(\"cpu\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "36756296"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from typing import Iterable, List\n",
    "# Define special symbols and indices, <unk> symbol will be in the last positions\n",
    "PAD_IDX, BOS_IDX, EOS_IDX, URL_IDX, EMAIL_IDX, PHONE_IDX, TGT_UNK_IDX = 0, 1, 2, 3, 4, 5, 6\n",
    "# TGT_UNK are for the tokens that neither appear in the vocab nor the text\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, num_position_markers = 1):\n",
    "        assert num_position_markers >= 1\n",
    "        self.num_position_markers = num_position_markers\n",
    "\n",
    "\n",
    "    def build_vocab(self, \n",
    "                    data_iter: Iterable,\n",
    "                    vocab_size: int):\n",
    "        token_counter = Counter()\n",
    "        for tokens in tqdm(yield_tokens(data_iter)):\n",
    "            token_counter.update(tokens)\n",
    "        # Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "        special_symbols = ['<pad>', '<bos>', '<eos>', '<url>', '<email>', '<phone>', \"<tgt_unk>\"]\n",
    "        self.vocab = special_symbols + list(sorted(token_counter.keys(), key = lambda x: -token_counter[x]))[:vocab_size]\n",
    "\n",
    "        self.word2index = {self.vocab[index]:index for index in range(len(self.vocab))}\n",
    "        self.special_symbols = special_symbols\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.vocab) + self.num_position_markers\n",
    "\n",
    "\n",
    "    def _lookup_index(self, token: str, position = 0):\n",
    "        # position should start from 0\n",
    "        assert position is None or position < self.num_position_markers\n",
    "        if token in self.word2index: return self.word2index[token]\n",
    "        else: \n",
    "            if position is not None:\n",
    "                return len(self.vocab) + position\n",
    "            else: return TGT_UNK_IDX\n",
    "\n",
    "\n",
    "    def lookup_indices(self, tokens: List[str], src_tokens: List[str] = None) -> List[int]:\n",
    "        assert hasattr(self, \"vocab\"), \"Vocab has not been built\"\n",
    "        if self.num_position_markers == 1:\n",
    "            # disregard the position of oov token, map to the same index (index of <unk>)\n",
    "            indices = [self._lookup_index(token) for token in tokens]\n",
    "        else:\n",
    "            # regard the position of oov token\n",
    "            indices = []\n",
    "            cache = {}\n",
    "            for i in range(len(tokens)):\n",
    "                token = tokens[i]\n",
    "                if token in self.special_symbols:\n",
    "                    indices.append(self.special_symbols.index(token))\n",
    "                    continue\n",
    "                if token not in cache:\n",
    "                    position = src_tokens.index(token) if ((src_tokens is not None) and (token in src_tokens)) == True else None\n",
    "                    token_index = self._lookup_index(token, position = position)\n",
    "                    cache[token] = token_index\n",
    "                indices.append(cache[token])\n",
    "        return indices\n",
    "\n",
    "    def lookup_token(self, index: int, src_tokens: List[int]):\n",
    "        if index < len(self.vocab): return self.vocab[index]\n",
    "        else:\n",
    "            if self.num_position_markers == 1:\n",
    "                # disregard position of oov token\n",
    "                return \"<unk>\"\n",
    "            else:\n",
    "                assert index - len(self.vocab) < self.num_position_markers\n",
    "                if src_tokens is None: \n",
    "                    return f\"<unk-{index - len(self.vocab)}>\"\n",
    "                else:\n",
    "                    return src_tokens[index - len(self.vocab)]\n",
    "    \n",
    "    \n",
    "    def lookup_tokens(self, indices: List[int], src_tokens: List[str] = None) -> List[str]:\n",
    "        assert hasattr(self, \"vocab\"), \"Vocab has not been built\"\n",
    "        return [self.lookup_token(index, src_tokens) for index in indices]\n",
    "\n",
    "CKPT_PATH = f\"/scratch/lamdo/unsupervised_keyphrase_prediction_2022/data/supervised_checkpoints/final/1/supervised.pth\"\n",
    "\n",
    "CKPT = torch.load(CKPT_PATH, map_location = torch.device(\"cpu\"))\n",
    "\n",
    "\n",
    "CONFIG = CKPT[\"config\"]\n",
    "\n",
    "\n",
    "num_params = sum(p.numel() for p in CKPT[\"transformer\"].values())\n",
    "\n",
    "num_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
